Basic Architecture:

OpenSearch is built on top of Apache Lucene.

Nodes store the data that we add to OpenSearch.
A cluster is a collection of nodes.
Each unit of data that we are going to store within OpenSearch cluster is called a document.
Documents are JSON objects containing whatever data we desire.
When we index a document, the original JSON object that we sent to OpenSearch is stored along with some metadata that OpenSearch uses internally.
Json
{
	"name": "John Doe",
	"country": "USA"
}
is stored as
{
	"_index": "people",
	"_type": "doc",
	"_id": "123",
	"_version": 1,
	"_seq_no": 0,
	"_primary_term": 1,
	"_source": {
		"name": "John Doe",
		"country": "USA"
	}
}
Documents are organized in OpenSearch cluster within indices. Every document within OpenSearch cluseter is stored within an index.
An index is a collection of documents that have similar characteristics and are logically related. (People Index => "John Doe", "Jane Doe")
When we get to searching for data, we will specify the index that we want to search for documents. (meaning search queries are actually run against indices).

=================================================================

Sharding and Scalability:

Sharding is a way to divide an index into separate pieces, where each piece is called shard. Sharding is done at the index level (not at the cluster or node level).
The main purpose for dividing an index into multiple shards, is to be able to horizontally scale the data volume.
Each shard is independent, and we can think of a shard as being a fully functional index on its own.
Each shard is actually Apache Lucene index.
The purpose of sharding
 - Mainly to be able to store more docs.
 - To easier fit large indices onto nodes.
 - Improved performance
  - Parallelization of queries increases the throughput of an index. (search query can be run on multiple shards at the same time)
  
GET _cat/indices?v
health status index                        uuid                   pri rep docs.count docs.deleted store.size pri.store.size
green  open   .plugins-ml-config           1ddOtvijTWyo3PBOvvZxfA   1   1          1            0      8.1kb            4kb
green  open   .ql-datasources              -sqY-Ww0TxS41TCLQT0mxg   1   1          0            0       416b           208b
green  open   .kibana_92668751_admin_1     h6yh72yIQLKNHdbDChvYNg   1   1          1            0     10.7kb          5.3kb
green  open   top_queries-2025.07.20-40856 lbKlCFE3Rmu9HJ7Fg80HJQ   1   1         20            4    236.4kb        119.1kb
green  open   security-auditlog-2025.07.20 lMKB-qiUSvy5H6mR0BPrGg   1   1        220            0      1.1mb        367.6kb
green  open   .opendistro_security         BpL5PUgVR1mGsMNmz1wWcA   1   1          9            0    162.2kb         81.1kb
green  open   .kibana_1                    0lrzDhcCS06LpHgOnlKoHQ   1   1          0            0       416b           208b

PRI stands for primary shards (1 - an index contains a single shard by default).

To increase the number of shards we need use the Split API for this. (it sill involves creating a new index).
To reduce the number of shards we need use the Shrink API.

Add a couple of shards for large indices; otherwise use default settings.
Sharding enables us to scale the data volume. Adding more nodes to the cluster helps too, but only to a certain extent (unless there are no very large indices).

=================================================================

Replication:

OpenSearch supports replication for fault tolerance. Replication is supported by default natively and enabled by default.
Replication is configured at the index level.
Replication works by creating copies of shards, reffered to as replicat shards.
A shard that has been replicated one or more times, is called a primary shard.
A primary shard and its replica shards are reffered to as a replication group.
Replica shards are a complete copy of a shard.
A replica shard can serve search requests, exactly like its primary shard.
The number of replicas can be configured at index creation.

An index contains shards, which in turn may contain replica shards. An index may be split into 2 primary shards, each having 2 replica shards.
The index, therefore, contains 2 replication groups.
Example index
 - Replication group A
  - Primary Shard A
	- Replica A1
	- Replica A2
 - Replication group B
  - Primary Shard B
	- Replica B1
	- Replica B2
Replica shards are never stored on the same node as their primary shard.
Replication only makes sense for clusters that contain more than 1 node.
For criticak systems, data should be replicated at least twice.

OpenSearch supports taking snapshots as backups.
Snapshots can be taken at the index level, or for the entire cluster.
Snapshots are used for backups, and replication for HA (and performance).

Create an index with default settings (1 Primary Shard and 1 Replica Shard)
Request:
PUT /pages
Response:
{
  "acknowledged": true,
  "shards_acknowledged": true,
  "index": "pages"
}

GET /_cat/indices?v
health status index                        uuid                   pri rep docs.count docs.deleted store.size pri.store.size
green  open   pages                        5Z3z6VFiQomhFOqyuXfabA   1   1          0            0       416b           208b

GET /_cat/shards?v
index                            shard prirep state   docs   store ip         node
pages                            0     p      STARTED    0    208b 172.19.0.2 opensearch-node2
pages                            0     r      STARTED    0    208b 172.19.0.4 opensearch-node1

=================================================================

Overview of node roles:
 - Master-eligible (the node may be elected as the cluster's master node)
	- Configuration: node.master: true|false
	- Master-node is responsible for creating and deleting indices, keeping track of nodes, allocating shards to node among others
 - Data - Enables a node to store data
	- Configuration: node.data: true|false
	- Storing data includes performing queries related to that data, such as search queries and modification of data.
 - Ingest - Enables a node to run ingest pipelines
	- Configuration: node.ingest: true|false
	- Ingest pipelines are a series of steps(processors) that are performed when indexing documents. Processors can manipulate documents before storing them to indexes.
 - Machine Learning
 - Coordination - refers to the distribution of queries and the aggregation of results.
	- Configuration: node.master: false, node.data: false, node.ingest: false, node.ml: false, xpack.ml.enabled: false
 - Voting-only - a node with this role will participate in the voting process for a new master node.
	- Configuration: node.voting_only: true|false
	
GET /_cat/nodes?v
ip         heap.percent ram.percent cpu load_1m load_5m load_15m node.role node.roles                                        cluster_manager name
172.19.0.2           68          40   0    0.04    0.11     0.09 dimr      cluster_manager,data,ingest,remote_cluster_client -               opensearch-node2
172.19.0.4           66          40   0    0.04    0.11     0.09 dimr      cluster_manager,data,ingest,remote_cluster_client *               opensearch-node1

=================================================================

Managing documents:

 - Create index with default settings:
PUT /pages
{
  "acknowledged": true,
  "shards_acknowledged": true,
  "index": "pages"
}

 - Create index with custom settings:
PUT /products
{
  "settings": {
    "number_of_shards": 1,
    "number_of_replicas": 1
  }
}
{
  "acknowledged": true,
  "shards_acknowledged": true,
  "index": "products"
}
GET /_cat/indices?v
health status index                        uuid                   pri rep docs.count docs.deleted store.size pri.store.size
green  open   products                     pIWXw-2PQpqtJfkgII-WIA   1   1          0            0       416b           208b

 - Delete index:
DELETE /pages

{
  "acknowledged": true
}

 - Indexing documents:
POST /products/_doc
{
  "name": "Coffee Maker",
  "price": 64,
  "in_stock": 10
  
}
{
  "_index": "products",
  "_id": "38GMLpgBGCdHRu0Wxuqj",
  "_version": 1,
  "result": "created",
  "_shards": {
    "total": 2, // 1 primary shard + 1 replica shard
    "successful": 2,
    "failed": 0
  },
  "_seq_no": 0,
  "_primary_term": 1
}

 - To indexing document with specific identifier:
PUT /products/_doc/100
{
  "name": "Toaster",
  "price": 49,
  "in_stock": 4
}
{
  "_index": "products",
  "_id": "100",
  "_version": 1,
  "result": "created",
  "_shards": {
    "total": 2,
    "successful": 2,
    "failed": 0
  },
  "_seq_no": 1,
  "_primary_term": 1
}

 - Retrieving documents by identifier:
GET /products/_doc/100
{
  "_index": "products",
  "_id": "100",
  "_version": 1,
  "_seq_no": 1,
  "_primary_term": 1,
  "found": true,
  "_source": {
    "name": "Toaster",
    "price": 49,
    "in_stock": 4
  }
}

 - Updating documents
POST /products/_update/100
{
  "doc": {
    "in_stock": 3
  }
}
{
  "_index": "products",
  "_id": "100",
  "_version": 2,
  "result": "updated",
  "_shards": {
    "total": 2,
    "successful": 2,
    "failed": 0
  },
  "_seq_no": 2,
  "_primary_term": 1
}

To add new field
POST /products/_update/100
{
  "doc": {
    "tags": ["electronics"]
  }
}
{
  "_index": "products",
  "_id": "100",
  "_version": 3,
  "result": "updated",
  "_shards": {
    "total": 2,
    "successful": 2,
    "failed": 0
  },
  "_seq_no": 3,
  "_primary_term": 1
}

OpenSearch documents are immutable. If we update/modify existing documents we actually replaced documents entirely.

 - Scripted updates:
POST /products/_update/100
{
  "script": {
    "source": "ctx._source.in_stock--"
  }
}
ctx - context
_source - current doc
in_stock - field of doc to update (substracting)

{
  "_index": "products",
  "_id": "100",
  "_version": 4,
  "result": "updated",
  "_shards": {
    "total": 2,
    "successful": 2,
    "failed": 0
  },
  "_seq_no": 4,
  "_primary_term": 1
}
GET /products/_doc/100
{
  "_index": "products",
  "_id": "100",
  "_version": 4,
  "_seq_no": 4,
  "_primary_term": 1,
  "found": true,
  "_source": {
    "name": "Toaster",
    "price": 49,
    "in_stock": 2,
    "tags": [
      "electronics"
    ]
  }
}

POST /products/_update/100
{
  "script": {
    "source": "ctx._source.in_stock = 10"
  }
}
{
  "_index": "products",
  "_id": "100",
  "_version": 5,
  "result": "updated",
  "_shards": {
    "total": 2,
    "successful": 2,
    "failed": 0
  },
  "_seq_no": 5,
  "_primary_term": 1
}
GET /products/_doc/100
{
  "_index": "products",
  "_id": "100",
  "_version": 5,
  "_seq_no": 5,
  "_primary_term": 1,
  "found": true,
  "_source": {
    "name": "Toaster",
    "price": 49,
    "in_stock": 10,
    "tags": [
      "electronics"
    ]
  }
}

Example with params:
POST /products/_update/100
{
  "script": {
    "source": "ctx._source.in_stock -= params.quantity",
    "params": {
      "quantity": 4
    }
  }
}
{
  "_index": "products",
  "_id": "100",
  "_version": 6,
  "result": "updated",
  "_shards": {
    "total": 2,
    "successful": 2,
    "failed": 0
  },
  "_seq_no": 6,
  "_primary_term": 1
}

GET /products/_doc/100
{
  "_index": "products",
  "_id": "100",
  "_version": 6,
  "_seq_no": 6,
  "_primary_term": 1,
  "found": true,
  "_source": {
    "name": "Toaster",
    "price": 49,
    "in_stock": 6,
    "tags": [
      "electronics"
    ]
  }
}

// noop example if in_stock items equals zero (no changes applies to doc, otherwise doc will be updated)
POST /products/_update/100
{
  "script": {
    "source": """
      if (ctx._source.in_stock == 0) {
        ctx.op = 'noop';
      }
      ctx._source.in_stock--;
    """
    }
  }
}
{
  "_index": "products",
  "_id": "100",
  "_version": 7,
  "result": "updated",
  "_shards": {
    "total": 2,
    "successful": 2,
    "failed": 0
  },
  "_seq_no": 7,
  "_primary_term": 1
}
GET /products/_doc/100
{
  "_index": "products",
  "_id": "100",
  "_version": 21,
  "_seq_no": 21,
  "_primary_term": 1,
  "found": true,
  "_source": {
    "name": "Toaster",
    "price": 49,
    "in_stock": 0,
    "tags": [
      "electronics"
    ]
  }
}
{
  "_index": "products",
  "_id": "100",
  "_version": 21,
  "result": "noop",
  "_shards": {
    "total": 0,
    "successful": 0,
    "failed": 0
  },
  "_seq_no": 21,
  "_primary_term": 1
}

// remove product when in_stock less than 1
POST /products/_update/100
{
  "script": {
    "source": """
      if (ctx._source.in_stock <= 1) {
        ctx.op = 'delete';
      }
      ctx._source.in_stock--;
    """
    }
  }
}
{
  "_index": "products",
  "_id": "100",
  "_version": 22,
  "result": "deleted",
  "_shards": {
    "total": 2,
    "successful": 2,
    "failed": 0
  },
  "_seq_no": 22,
  "_primary_term": 1
}

GET /products/_doc/100
{
  "_index": "products",
  "_id": "100",
  "found": false
}

 - Upserts:
Upserting means to conditionally update or insert a doc based on whether or not the doc already exists.
So, if doc already exists, a script is run, and if it doesn't, a new doc is indexed
POST /products/_update/101  
{
  "script": {
    "source": "ctx._source.in_stock++"
  },
  "upsert": {
    "name": "Blender",
    "price": 399,
    "in_stock": 5
  }
}
{
  "_index": "products",
  "_id": "101",
  "_version": 1,
  "result": "created", // new document is created
  "_shards": {
    "total": 2,
    "successful": 2,
    "failed": 0
  },
  "_seq_no": 27,
  "_primary_term": 1
}
GET /products/_doc/101
{
  "_index": "products",
  "_id": "101",
  "_version": 1,
  "_seq_no": 27,
  "_primary_term": 1,
  "found": true,
  "_source": {
    "name": "Blender",
    "price": 399,
    "in_stock": 5
  }
}
execute 1 more time
POST /products/_update/101  
{
  "script": {
    "source": "ctx._source.in_stock++"
  },
  "upsert": {
    "name": "Blender",
    "price": 399,
    "in_stock": 5
  }
}
{
  "_index": "products",
  "_id": "101",
  "_version": 2,
  "result": "updated",
  "_shards": {
    "total": 2,
    "successful": 2,
    "failed": 0
  },
  "_seq_no": 28,
  "_primary_term": 1
}
GET /products/_doc/101
{
  "_index": "products",
  "_id": "101",
  "_version": 2,
  "_seq_no": 28,
  "_primary_term": 1,
  "found": true,
  "_source": {
    "name": "Blender",
    "price": 399,
    "in_stock": 6 // chanded due to the script
  }
}

 - Replacing docs
PUT /products/_doc/100
{
  "name": "Toaster",
  "price": 49,
  "in_stock": 4
}
{
  "_index": "products",
  "_id": "100",
  "_version": 5,
  "result": "updated",
  "_shards": {
    "total": 2,
    "successful": 2,
    "failed": 0
  },
  "_seq_no": 29,
  "_primary_term": 1
}

GET /products/_doc/100
{
  "_index": "products",
  "_id": "100",
  "_version": 5,
  "_seq_no": 29,
  "_primary_term": 1,
  "found": true,
  "_source": {
    "name": "Toaster",
    "price": 49,
    "in_stock": 4
  }
}

 - Deleting docs
DELETE /products/_doc/101
{
  "_index": "products",
  "_id": "101",
  "_version": 3,
  "result": "deleted",
  "_shards": {
    "total": 2,
    "successful": 2,
    "failed": 0
  },
  "_seq_no": 30,
  "_primary_term": 1
}

=================================================================

Routing:

Routing is the process of resolving a shard for a doc.
shard_num = hash(_routing) % num_primary_shards; // by default _routing = doc _id ("_id": "101"). The default rouring strategy distributes documents evenly
If we decided to override routing strategy we will see "_routing" metadata in the response.

If we were to change the number of shards for an index, then the routing formula would yield different result. (it will be cause for the existing docs to retrieve them).
Reminder: Modifying the number of shards requires creating a new index and reindexing docs into it (Shrink and Split APIs)

=================================================================

Write request is routed to a primary shard, which then validates the operation, performs it locally, and distributes it to its replica shards in parallel.

{
  "_index": "products",
  "_id": "100",
  "_version": 5,
  "_seq_no": 29,
  "_primary_term": 1,
  "found": true,
  "_source": {
    "name": "Toaster",
    "price": 49,
    "in_stock": 4
  }
}

A _primary_term is a way to distinguish between old and new primary shards. Essentially a counter for how many times a primary shard has changed. The primary_term appeneds to write ops.
A _seq_no appends to write ops. Counter that will incremented for each write ops. The primary shard increases the sequence number.

Recovering when primary shards fails:
 - Primary terms and sequence number are key when OpenSearch needs to recover from a primary shard failure.
 - Instead of having to compare data on disk, it can use primary terms and sequence numbers to figure out on which operation have already been performed, and which are needed to bring a given shard to up to date.

=================================================================

Optimistic concurrency control:
POST /products/_update/100?if_primary_term=1&if_seq_no=71

Example:
GET /products/_doc/100
{
  "_index": "products",
  "_id": "100",
  "_version": 6,
  "_seq_no": 31,
  "_primary_term": 1,
  "found": true,
  "_source": {
    "name": "Toaster",
    "price": 49,
    "in_stock": 3
  }
}

Update Request:
POST /products/_update/100?if_primary_term=1&if_seq_no=32
{
  "doc": {
    "in_stock": 3
  }
}
Error Message:
{
  "error": {
    "root_cause": [
      {
        "type": "version_conflict_engine_exception",
        "reason": "[100]: version conflict, required seqNo [32], primary term [1]. current document has seqNo [31] and primary term [1]",
        "index": "products",
        "shard": "0",
        "index_uuid": "pIWXw-2PQpqtJfkgII-WIA"
      }
    ],
    "type": "version_conflict_engine_exception",
    "reason": "[100]: version conflict, required seqNo [32], primary term [1]. current document has seqNo [31] and primary term [1]",
    "index": "products",
    "shard": "0",
    "index_uuid": "pIWXw-2PQpqtJfkgII-WIA"
  },
  "status": 409
}

Handle failure situation at the application level
 = Retrieve the doc again
 - Use _primary_term and _seq_no again for new update request
=================================================================

Update by query:

Updates multiple docs within a signle query (similar UPDATE WHERE query in RDBMS)
The query uses 3 concepts (primary terms, sequence numbers, optimistic concurrency control)
The query creates a snapshot to do optimistic concurrency control
Search queries and bulk updates are sent to replication group sequentially
 - OpenSearch retries these queries up to 10 times
 - If the queries still fail, the whole query is aborted
  - Any changes already made to docs, are not rolled back
API returns info about failures
If the document has been modified since taking the snapshot, the query is aborted
 - This is checked with the docs primary terms and sequence number
To count version conflicts instead of aborting the query, the "conflicts" option can be set to "proceed"


GET /products/_search
{
  "query": {
    "match_all": {}
  }
}
Response:
{
  "took": 2,
  "timed_out": false,
  "_shards": {
    "total": 1,
    "successful": 1,
    "skipped": 0,
    "failed": 0
  },
  "hits": {
    "total": {
      "value": 2,
      "relation": "eq"
    },
    "max_score": 1,
    "hits": [
      {
        "_index": "products",
        "_id": "100",
        "_score": 1,
        "_source": {
          "name": "Toaster",
          "price": 49,
          "in_stock": 3
        }
      },
      {
        "_index": "products",
        "_id": "38GMLpgBGCdHRu0Wxuqj",
        "_score": 1,
        "_source": {
          "name": "Coffee Maker",
          "price": 64,
          "in_stock": 10
        }
      }
    ]
  }
}

Update by query example:
POST /products/_update_by_query
{
  "script": {
    "source": "ctx._source.in_stock--"
  },
  "query": {  // define which docs will be affected within update by query
    "match_all": {} // run the script for all docs which match the query
  }
}
Response:
{
  "took": 247,
  "timed_out": false,
  "total": 2,  // 2 docs were updated
  "updated": 2,
  "deleted": 0,
  "batches": 1,
  "version_conflicts": 0,
  "noops": 0,
  "retries": {
    "bulk": 0,
    "search": 0
  },
  "throttled_millis": 0,
  "requests_per_second": -1,
  "throttled_until_millis": 0,
  "failures": []
}

=================================================================

Delete by query:

Delete multiple docs withing a single query
POST /products/_delete_by_query
{
  "query": {
    "match_all": {} // delete all docs are matched to query
  }
}
Response:
{
  "took": 26,
  "timed_out": false,
  "total": 2,
  "deleted": 2,
  "batches": 1,
  "version_conflicts": 0,
  "noops": 0,
  "retries": {
    "bulk": 0,
    "search": 0
  },
  "throttled_millis": 0,
  "requests_per_second": -1,
  "throttled_until_millis": 0,
  "failures": []
}

=================================================================

Batch processing:

Performs index, create, update, delete on many docs with a single query using Bulk API. 
(the diff between index and create is create action will fail if the doc already exists which is not the case for the index action)
(if we use the index action the doc will be added if it doesn't already exist, otherwise it will be replaced)
POST /_bulk
{ "index": { "_index": "products", "_id": 200 } }
{ "name": "Espresso Machine", "price": 199, "in_stock": 5 }
{ "create": { "_index": "products", "_id": 200 } }
{ "name": "Espresso Machine", "price": 199, "in_stock": 5 }
Response:
{
  "took": 13,
  "errors": true,
  "items": [
    {
      "index": {
        "_index": "products",
        "_id": "200",
        "_version": 2,
        "result": "updated",
        "_shards": {
          "total": 2,
          "successful": 2,
          "failed": 0
        },
        "_seq_no": 37,
        "_primary_term": 3,
        "status": 200
      }
    },
    {
      "create": {
        "_index": "products",
        "_id": "200",
        "status": 409,
        "error": {
          "type": "version_conflict_engine_exception",
          "reason": "[200]: version conflict, document already exists (current version [2])",
          "index": "products",
          "shard": "0",
          "index_uuid": "pIWXw-2PQpqtJfkgII-WIA"
        }
      }
    }
  ]
}

Correct usage:
POST /_bulk
{ "index": { "_index": "products", "_id": 200 } }
{ "name": "Espresso Machine", "price": 199, "in_stock": 5 }
{ "create": { "_index": "products", "_id": 201 } }
{ "name": "Milk Frother", "price": 149, "in_stock": 14 }
Response:
{
  "took": 5,
  "errors": false,
  "items": [
    {
      "index": {
        "_index": "products",
        "_id": "200",
        "_version": 1,
        "result": "updated",
        "_shards": {
          "total": 2,
          "successful": 2,
          "failed": 0
        },
        "_seq_no": 38,
        "_primary_term": 3,
        "status": 201
      }
    },
    {
      "create": {
        "_index": "products",
        "_id": "201",
        "_version": 1,
        "result": "created",
        "_shards": {
          "total": 2,
          "successful": 2,
          "failed": 0
        },
        "_seq_no": 39,
        "_primary_term": 3,
        "status": 201
      }
    }
  ]
}

The Bulk API expects data formatted using the NDJSON specification:
action_and_metadata\n
optional_source\n
action_and_metadata\n
optional_source\n


POST /_bulk
{
  "index": { // action
    "_index": "products", // metadata
    "_id": 200
  }
}
{
  "name": "Espresso Machine", // source document
  "price": 199,
  "in_stock": 5
}


POST /_bulk
{ "update": { "_index": "products", "_id": 201 } }
{ "doc": { "price": 129 } }
{ "delete": { "_index": "products", "_id": 200 } }

or
POST /products/_bulk - the actions are now all run against "products" index so that means we can remove them from metadata.
{ "update": { "_id": 201 } }
{ "doc": { "price": 129 } }
{ "delete": { "_id": 200 } }
Response:
{
  "took": 9,
  "errors": false,
  "items": [
    {
      "update": {
        "_index": "products",
        "_id": "201",
        "_version": 6,
        "result": "updated",
        "_shards": {
          "total": 2,
          "successful": 2,
          "failed": 0
        },
        "_seq_no": 47,
        "_primary_term": 3,
        "status": 200
      }
    },
    {
      "delete": {
        "_index": "products",
        "_id": "200",
        "_version": 7,
        "result": "deleted",
        "_shards": {
          "total": 2,
          "successful": 2,
          "failed": 0
        },
        "_seq_no": 48,
        "_primary_term": 3,
        "status": 200
      }
    }
  ]
}

Notes:
 - The HTTP header Content-Type should be set as Content-Type: application/x-ndjson
 - A failed action will not affect other actions
 
 
curl --location 'https://localhost:9200/products/_bulk' \
--header 'Content-Type: application/x-ndjson' \
--header 'Authorization: ••••••' \
--data-binary '@/C:/Users/uladz/Downloads/products-bulk.json'

GET /_cat/indices?v
health status index                        uuid                   pri rep docs.count docs.deleted store.size pri.store.size
green  open   products                     svq38Li4QZ6uDp6GtvKM9w   1   1       1000            0    744.4kb        387.8kb
GET /_cat/shards?v
index                            shard prirep state   docs   store ip         node
products                         0     p      STARTED 1000 387.8kb 172.19.0.3 opensearch-node2
products                         0     r      STARTED 1000 356.5kb 172.19.0.4 opensearch-node1

=================================================================

Analysis (text analysis):

Text values are analyzed when indexing docs.
The result is stored in data structures that are efficient for searching.
The "_source" object is not used when searching for documents (It contains exact values specified when indexing a doc).

When a text value is indexed, analyzer is used to process the text (Character Filters, Tokenizer, Token Filters).
The result of analyzed text values are stored in a searchable data structure.

Character Filters:
 - Receive the original text and may transform it by adding, removing or changing characters.
 - Analyzer contains 0 or more character filters.
 - Character Filters are applied in the order in which they are specified.
Example (html_strip filter):
 - Input "I&apos;m in a <em>good</em> mood&nbsp;-&nbsp;and <strong>love</strong> acai!"
 - Output "I'm in a good mood - and love acai!"
 
Tokenizers:
 - An analyzer contains 1 Tokenizer that resonsible for tokenizing the text.
 - Tokenizes a string, i.e. splits it into tokens
 - Characters may be stripped as part of the tokenization.
Example
 - Input: "I REALLY like beer!"
 - Output: "["I", "REALLY", "like", "beer"]" // tokenizer also records the character offsets for each token in the original string.
 
Token Filters:
 - Receive the output of the tokenizer as input (i.e. the tokens)
 - A token filter can add, remove, modify tokens
 - An analyzer contains 0 or more token filters
 - Token filters are applied in the order in which they are specified
Example (lowercase filter)
 - Input: "["I", "REALLY", "like", "beer"]"
 - Output: "["i", "really", "like", "beer"]"

=================================================================

Using the Analyze API

POST /_analyze
{
  "text": "2 guys walk into  a bar, but the third... DUCKS! :-)",
  "analyzer": "standard" // removes .../,/!/etc , and trims whitespaces, uses lowercase token filter
}
Response:
{
  "tokens": [
    {
      "token": "2",
      "start_offset": 0, // offsets where the token appeared in the text before it was analyzed.
      "end_offset": 1,
      "type": "<NUM>",
      "position": 0
    },
    {
      "token": "guys",
      "start_offset": 2,
      "end_offset": 6,
      "type": "<ALPHANUM>",
      "position": 1
    },
    {
      "token": "walk",
      "start_offset": 7,
      "end_offset": 11,
      "type": "<ALPHANUM>",
      "position": 2
    },
    {
      "token": "into",
      "start_offset": 12,
      "end_offset": 16,
      "type": "<ALPHANUM>",
      "position": 3
    },
    {
      "token": "a",
      "start_offset": 18,
      "end_offset": 19,
      "type": "<ALPHANUM>",
      "position": 4
    },
    {
      "token": "bar",
      "start_offset": 20,
      "end_offset": 23,
      "type": "<ALPHANUM>",
      "position": 5
    },
    {
      "token": "but",
      "start_offset": 25,
      "end_offset": 28,
      "type": "<ALPHANUM>",
      "position": 6
    },
    {
      "token": "the",
      "start_offset": 29,
      "end_offset": 32,
      "type": "<ALPHANUM>",
      "position": 7
    },
    {
      "token": "third",
      "start_offset": 33,
      "end_offset": 38,
      "type": "<ALPHANUM>",
      "position": 8
    },
    {
      "token": "ducks",
      "start_offset": 42,
      "end_offset": 47,
      "type": "<ALPHANUM>",
      "position": 9
    }
  ]
}
or
POST /_analyze
{
  "text": "2 guys walk into  a bar, but the third... DUCKS! :-)",
  "char_filter": [],
  "tokenizer": "standard",
  "filter": ["lowercase"]
}
Response will be the same:
{
  "tokens": [
    {
      "token": "2",
      "start_offset": 0,
      "end_offset": 1,
      "type": "<NUM>",
      "position": 0
    },
    {
      "token": "guys",
      "start_offset": 2,
      "end_offset": 6,
      "type": "<ALPHANUM>",
      "position": 1
    },
    {
      "token": "walk",
      "start_offset": 7,
      "end_offset": 11,
      "type": "<ALPHANUM>",
      "position": 2
    },
    {
      "token": "into",
      "start_offset": 12,
      "end_offset": 16,
      "type": "<ALPHANUM>",
      "position": 3
    },
    {
      "token": "a",
      "start_offset": 18,
      "end_offset": 19,
      "type": "<ALPHANUM>",
      "position": 4
    },
    {
      "token": "bar",
      "start_offset": 20,
      "end_offset": 23,
      "type": "<ALPHANUM>",
      "position": 5
    },
    {
      "token": "but",
      "start_offset": 25,
      "end_offset": 28,
      "type": "<ALPHANUM>",
      "position": 6
    },
    {
      "token": "the",
      "start_offset": 29,
      "end_offset": 32,
      "type": "<ALPHANUM>",
      "position": 7
    },
    {
      "token": "third",
      "start_offset": 33,
      "end_offset": 38,
      "type": "<ALPHANUM>",
      "position": 8
    },
    {
      "token": "ducks",
      "start_offset": 42,
      "end_offset": 47,
      "type": "<ALPHANUM>",
      "position": 9
    }
  ]
}

=================================================================

Understanding inverted indices

 - A field's values are stored in 1 of several data structures
   - The data structure depends on the field's data type
 - Ensures efficient data access - e.g. searches
 - Handled by Apache Lucene
 
Inverted indices 
 - is essentially a mapping between terms and which docs contain them
 - Outside the context of analyzers, we use the terminology "terms" - tokens that are emitted by the analyzer
 
Sentence 												->	Tokens
"2 guys walk into  a bar, but the third... DUCKS! :-)" 	->	["2", "guys", "walk", "into", "a", "bar", "but", "the", "third", "ducks"]
"2 guys want into a bar"								->	["2", "guys", "went", "into", "a", "bar"]
"2 ducks walk around the lake"							->	["2", "ducks", "walk", "around", "the", "lake"]

Term is the smallest unit of search stored in the inverted index.
Terms are generated from text after it has been processed by an analyzer.

Term	Document #1		Document #2		Document #3
2			x				x				x
a			x				x				
around										x
bar			x				x
but			x
ducks		x								x                                 <----- Inverted index
guys		x				x
into		x				x
lake										x
the			x								x	
third		x				
walk		x								x
went						x

Example
 - we are looking for search "ducks" -> figuring out which docs are contain that term("token") by performing simple lookup in the inverted index
 - doc 1 and 3 contain this term (that makes the process of searching for term more simpler and efficient).
 
The inverted indices that are stored within Apache Lucene contain a bit more information, such as data that is used for relevance scoring

Lets assume that docs have a number of different fields:
Doc 1
{
 "name": "Coffee Machine",
 "description": "Makes coffee super fast!"
 "price": 64,
 "in_stock": 10,
 "created_at": "2025-05-05" 
}

Doc 2
{
 "name": "Toaster",
 "description": "Makes delicious toasts..."
 "price": 49,
 "in_stock": 4,
 "created_at": "2017-06-06" 
}

An inverted index is actually created for each text field, meaning that we will have 2 inverted indices for the "name" and "description" fields ("name" and "description" fields are textual)
name field
TERM		Doc 1		Doc 2
coffee		x
maker		x
toaster					x

description field
TERM 		Doc 1		Doc 2
coffee		x
delicious				x
fast		x
makes		x			x
super		x
toasts					x

NOTE: One inverted index per textual field. Other data types use BKD trees, for instance numeric, date, geospatial. BKD is efficient for the range numeric queries.

=================================================================

Mapping

Defines	the structure of docs (e.g. fields and their data types)
Also used to configure how values are indexed.
Similar to a tables schema in RDBMS
PUT /employees
{
  "mappings": {
	"properties": {
	  "id": { "type": "integer" },
	  "first_name": { "type": "text" },
	  "last_name": { "type": "text" },
	  "dob": { "type": "date" },
	  "description": { "type": "text" },
	  "created_at": { "type": "date" }
	}
  }
}

Explicit mappings - we define field and data type mappings ourselves, typically when creating an index.
Dynamic mappings - OpenSearch generates field mappings for us, field mapping will be automatically created when OpenSearch encounters a new field.

=================================================================

Overview of data types

https://www.elastic.co/docs/reference/elasticsearch/mapping-reference/field-data-types
https://docs.opensearch.org/latest/field-types/supported-field-types/index/

Basic list of data types:
 - object 
	- Used for any JSON object (each doc that we index into OpenSearch is a JSON object)
	- Objects may be nested
	- Mapped using the "properties" parameter
	  {
		"name": "Coffee Maker",
		"manufacturer": {
		  "country": "China"
		}
	  }
	  PUT /products
	  {
		"mappings": {
		  "properties": { // properties key is added for objects instead of specifying the type key as with other data types. 
		    "name": { "type": "text" },
			"manufacturer": {
			  "properties": {
			    "country": { "type": "text" }
			  }
			}
		  }
		}
	  }
	- Objects are not stored as objects in Apache Lucene
		- Objects are transformed to ensure that we can index any valid JSON
		- In particular, objects are flattened
		  {
			"name": "Coffee Maker",
			"manufacturer": {
			  "country": "China"
			}
		  }
		  Transform to 
		  {
		    "name": "Coffee Maker",
			"manufacturer.country": "China"
		  }
		  Example with flattened array: 
		  {
		    "name": "Coffee Maker",
			"reviews.rating": [5.0, 3.5],
			"reviews.author": ["John Doe", "Jane Doe"]
			"reviews.description": [
			  "Great",
			  "Cool"
			]
		  }
 - nested
	- Similar to the "object" data type, but maintains object relationships
		- Useful when indexing arrays of objects (objects are stored independently)
	- Enables us to query objects independently
		- Must use the nested query
	{
	  "name": "Coffee Maker",
	  "reviews": [
	    {
		  "rating": 5.0,
		  "author": "John Doe",
		  "description": "Great"
		},
		{
		  "rating": 3.5,
		  "author": "Jane Doe",
		  "description": "Cool"
		}
	  ]
	}
	PUT /products
	{
	  "mappings": {
	    "properties"": {
		  "name": { "type": "text" },
		  "reviews": { "type": "nested" }
		}
	  }
	}
	- nested objects are stored as hidden docs (for example: 1 doc as product and 2 docs as review)
 - keyword
	- used for exact matching of values (e.g. email like in example below)
	- typically used for filtering, aggregations and sorting (e.g. searching for articles with a status PUBLISHED)
	- for full-text searches, use the "text" data type instead. (full-text searches are searches that do not require exact matches).
	  (e.g. searching the body text of an article).
	- keyword fields are analyzed with the keyword analyzer
	- the keyword analyzer is a no-op analyzer (it outputs the unmodified string as a single token).
	  POST /_analyze
	  {
        "text": "2 guys walk into  a bar, but the third... DUCKS! :-)",
        "analyzer": "keyword"
      }
	  Response:
	  {
		"tokens": [
			{
				"token": "2 guys walk into  a bar, but the third... DUCKS! :-)",
				"start_offset": 0,
				"end_offset": 52,
				"type": "word",
				"position": 0
			}
		]
	  }
	  Inverted Index example:
		TERM														Doc 1		Doc 2
		2 guys walk into  a bar, but the third... DUCKS! :-)		x
		2 guys went into a bar													x
	  Inverted Index example 2:
	  { 
	    "name": " John Doe",
	    "email": "johndoe@example.com"
	  }
	  { 
	    "name": " Jane Doe",
	    "email": "janedoe@example.com"
	  }
		TERM					Doc 1		Doc 2
		johndoe@example.com		x
		janedoe@example.com					x
 - integer
 - long
 - boolean
 - text
 - double
 - short
 - date
 - float

=================================================================

Understanding type coercion

 - Data types are inspected when indexing docs (validates and some invalid values will be rejected)
 PUT /coercion_test/_doc/1
{
  "price": 7.4 // float
}

PUT /coercion_test/_doc/2
{
  "price": "7.4" // text will be use type coercion "string" to "float"
}

PUT /coercion_test/_doc/3
{
  "price": "7.4m"
}
Response:
{
  "error": {
    "root_cause": [
      {
        "type": "mapper_parsing_exception",
        "reason": "failed to parse field [price] of type [float] in document with id '3'. Preview of field's value: '7.4m'"
      }
    ],
    "type": "mapper_parsing_exception",
    "reason": "failed to parse field [price] of type [float] in document with id '3'. Preview of field's value: '7.4m'",
    "caused_by": {
      "type": "number_format_exception",
      "reason": "For input string: \"7.4m\""
    }
  },
  "status": 400
}

Coercion mechanism inspects mapping and coerce into field data if possible.
GET /coercion_test
{
  "coercion_test": {
    "aliases": {},
    "mappings": {
      "properties": {
        "price": {
          "type": "float"
        }
      }
    },
    "settings": {
      "index": {
        "replication": {
          "type": "DOCUMENT"
        },
        "number_of_shards": "1",
        "provided_name": "coercion_test",
        "creation_date": "1753835469901",
        "number_of_replicas": "1",
        "uuid": "Ld3ZXOCPSICDGxLn56QnUA",
        "version": {
          "created": "137227827"
        }
      }
    }
  }
}

Understanding the "_source" object:
 - Contains the values that were supplied at index time ("7.4")
	- not the values that are indexed
 - Search queries use indexed values, not "_source"
GET /coercion_test/_doc/2
Response:
{
  "_index": "coercion_test",
  "_id": "2",
  "_version": 3,
  "_seq_no": 3,
  "_primary_term": 1,
  "found": true,
  "_source": {
    "price": "7.4"
  }
}
 - "_source" does not reflect how values are indexed
  - Keep coercion in mind if we use values from "_source" (in the example above it might be either a string or floating point)
 - Disabling coercion is a matter of preference (it is enabled by default, but good practice to disable it).
How to double-check at index level
Run:
GET /coercion_test/_settings
Look for:
"index.mapping.coerce": "false" // If not present → coercion is enabled globally.

=================================================================

Understanding arrays

 - There are no such thing as an array data type
 - Any field may contain 0 or more values by default

{
	"tags": "Smartphone"
}
handling the same like because the tags will be mapped as "text"
{
	"tags": [ "Smartphone", "Electronics" ] // it will concatinated before analyzing and resulting tokens are stored within inverted index as normal
}
Mapping
{
	"products": {
		"mappings": {
			"properties": {
				"tags": {
					"type": "text"
				}
			}
		}
	}
}

POST /_analyze
{
  "text": ["Strings are simply", "merged together"],
  "analyzer": "standard"
}
Response:
{
  "tokens": [
    {
      "token": "strings",
      "start_offset": 0,
      "end_offset": 7,
      "type": "<ALPHANUM>",
      "position": 0
    },
    {
      "token": "are",
      "start_offset": 8,
      "end_offset": 11,
      "type": "<ALPHANUM>",
      "position": 1
    },
    {
      "token": "simply",
      "start_offset": 12,
      "end_offset": 18,
      "type": "<ALPHANUM>",
      "position": 2
    },
    {
      "token": "merged",
      "start_offset": 19,
      "end_offset": 25,
      "type": "<ALPHANUM>",
      "position": 3
    },
    {
      "token": "together",
      "start_offset": 26,
      "end_offset": 34,
      "type": "<ALPHANUM>",
      "position": 4
    }
  ]
}

In the case of non-text fields, the values are not analyzed, and multiple values are just stored within the appropriate data structure within Apache Lucene.
Constraints:
 - Array values should be of the same data type (coercion will be work - need to be care)
 - Coercion only works for fields that are already mapped

Nested arrays
 - Arrays may contain nested arrays
 - Arrays are flattened during indexing
 - [ 1, [ 2, 3 ] ] => [ 1, 2, 3 ]

Remember to use the nested data type for arrays of objects if you need to query the objects independently.

=================================================================

Adding explicit mappings

PUT /reviews
{
  "mappings": {
    "properties": {
      "rating": { "type": "float" },
      "content": { "type": "text" },
      "product_id": { "type": "integer" },
        "author": {
          "properties": {
            "first_name": { "type": "text" },
            "last_name": { "type": "text" },
            "email": { "type": "keyword" }
          }
        }
    }
  }
}
Response:
{
  "acknowledged": true,
  "shards_acknowledged": true,
  "index": "reviews"
}

PUT /reviews/_doc/1
{
  "rating": 5.0,
  "content": "Outstanding course! Bo really taught me a lot about OpenSearch!",
  "product_id": 123,
  "author": {
    "first_name": "John",
    "last_name": "Doe",
    "email": "johndoe@example.com"
  }
}


PUT /reviews/_doc/666
{
  "rating": 1.0,
  "content": "Hate OpenSearch!",
  "product_id": "123", // Coercion because it didn't disable by default
  "author": {
    "first_name": "Jane",
    "last_name": "Doe",
    "email": "janedoe@example.com"
  }
}

=================================================================

Retrieving mappings

Useful when we are used dynamic mapping during index creation and firstly indexing the doc.

GET /reviews/_mapping/
Response:
{
  "reviews": {
    "mappings": {
      "properties": {
        "author": {
          "properties": {
            "email": {
              "type": "keyword"
            },
            "first_name": {
              "type": "text"
            },
            "last_name": {
              "type": "text"
            }
          }
        },
        "content": {
          "type": "text"
        },
        "product_id": {
          "type": "integer"
        },
        "rating": {
          "type": "float"
        }
      }
    }
  }
}

Also possible to retrieve mapping for the specific field
GET /reviews/_mapping/field/author.email
Response:
{
  "reviews": {
    "mappings": {
      "author.email": {
        "full_name": "author.email",
        "mapping": {
          "email": {
            "type": "keyword"
          }
        }
      }
    }
  }
}

=================================================================

Using dot-notation in field names
Instead of defying nested objects and their types using "properties"-approach for the nested fields it very handy to use dot-notation approach as field separation.

So, instead of
PUT /reviews
{
  "mappings": {
    "properties": {
      "rating": { "type": "float" },
      "content": { "type": "text" },
      "product_id": { "type": "integer" },
        "author": {
          "properties": {
            "first_name": { "type": "text" },
            "last_name": { "type": "text" },
            "email": { "type": "keyword" }
          }
        }
    }
  }
}
will use dot-notation for the author fields:
PUT /reviews_dot_notation
{
  "mappings": {
    "properties": {
      "rating": { "type": "float" },
      "content": { "type": "text" },
      "product_id": { "type": "integer" },
      "author.first_name": { "type": "text" },
      "author.last_name": { "type": "text" },
      "author.email": { "type": "keyword" }
    }
  }
}
Response:
{
  "acknowledged": true,
  "shards_acknowledged": true,
  "index": "reviews_dot_notation"
}

GET /reviews_dot_notation/_mapping/
Response:
{
  "reviews_dot_notation": {
    "mappings": {
      "properties": {
        "author": {
          "properties": {
            "email": {
              "type": "keyword"
            },
            "first_name": {
              "type": "text"
            },
            "last_name": {
              "type": "text"
            }
          }
        },
        "content": {
          "type": "text"
        },
        "product_id": {
          "type": "integer"
        },
        "rating": {
          "type": "float"
        }
      }
    }
  }
}

=================================================================

Adding mappings for the existing indices

given
{
  "mappings": {
    "properties": {
      "rating": { "type": "float" },
      "content": { "type": "text" },
      "product_id": { "type": "integer" },
        "author": {
          "properties": {
            "first_name": { "type": "text" },
            "last_name": { "type": "text" },
            "email": { "type": "keyword" }
          }
        }
    }
  }
}

For example we wanna add the "created_at" field for the review index
PUT /reviews/_mapping
{
  "properties": {
    "created_at": {
      "type": "date"
    }
  }
}

GET /reviews/_mapping/
Response:
{
  "reviews": {
    "mappings": {
      "properties": {
        "author": {
          "properties": {
            "email": {
              "type": "keyword"
            },
            "first_name": {
              "type": "text"
            },
            "last_name": {
              "type": "text"
            }
          }
        },
        "content": {
          "type": "text"
        },
        "created_at": {
          "type": "date"
        },
        "product_id": {
          "type": "integer"
        },
        "rating": {
          "type": "float"
        }
      }
    }
  }
}

=================================================================

How dates work in OpenSearch

 - Dates may specified in 1 of 3 ways:
	- speciall formatted strings
	- long (number in ms since the epoch)
	- integer (number in seconds since the epoch)
 - Epoch refers to the 1st of January 1970
 - Custom formats are supported

Default behavior od "date" fields
 - 3 supported formats:
	- A date without time
	- A date with time
	- Millis since the Epoch (long)
 - UTC timezone assumed of none is specified
 - Dates must be formatted according to the ISO 8601 specification

How "date" fields are stored in OpenSearch
 - Stored internally as ms since the epoch (long)
 - Any valid value that we supply at index time is covered to a long value internally
   Example: "created_at": "2015-04-15T13:07:14Z" -> 1429103261000
 - Dates are converted to the UTC timezone
 - The same date conversion happens for the search queries
   Example:
   GET /reviews/_search
   {
		"query": {
			"range": {
				"created_at": {
					"gte": "2015-04-15T13:07:14Z" -> 1429103261000 converted to the long value before search process begins
				}
			}
		}
   }
   
PUT /reviews/_doc/2
{
  "rating": 4.5,
  "content": "Not Bad. Not Bad at all!",
  "product_id": 123,
  "created_at": "2015-03-27", // check format
  "author": {
    "first_name": "Average",
    "last_name": "Doe",
    "email": "avgdoe@example.com"
  }
}

PUT /reviews/_doc/3
{
  "rating": 3.5,
  "content": "Could be better",
  "product_id": 123,
  "created_at": "2015-04-15T13:07:41Z", // check format T=time Z=UTC
  "author": {
    "first_name": "Spencer",
    "last_name": "Person",
    "email": "sperson@example.com"
  }
}

PUT /reviews/_doc/4
{
  "rating": 5.0,
  "content": "Incredible!",
  "product_id": 123,
  "created_at": "2015-01-28T09:21:51+01:00", // check format. The date is not UTC-format, instead of Z=UTC, UTC-offset can be specified (+01:00)
  "author": {
    "first_name": "Adam",
    "last_name": "Jones",
    "email": "ajones@example.com"
  }
}

PUT /reviews/_doc/5
{
  "rating": 4.5,
  "content": "Very useful",
  "product_id": 123,
  "created_at": 143601128400, // check format. Long number value (number of ms since the Epoch)
  "author": {
    "first_name": "Taylor",
    "last_name": "West",
    "email": "twest@example.com"
  }
}

Note: careful with coercion !!!

=================================================================

How missing fields are handled

GET /reviews/_search
{
  "query": {
    "match_all": {}
  }
}

{
  "took": 945,
  "timed_out": false,
  "_shards": {
    "total": 1,
    "successful": 1,
    "skipped": 0,
    "failed": 0
  },
  "hits": {
    "total": {
      "value": 6,
      "relation": "eq"
    },
    "max_score": 1,
    "hits": [
      {
        "_index": "reviews",
        "_id": "666",						// missed "created_at" field because we indexed this doc before "created_at" field has been introduced.
        "_score": 1,
        "_source": {
          "rating": 1,
          "content": "Hate OpenSearch!",
          "product_id": "123",
          "author": {
            "first_name": "Jane",
            "last_name": "Doe",
            "email": "janedoe@example.com"
          }
        }
      },
      {
        "_index": "reviews",
        "_id": "2",
        "_score": 1,
        "_source": {
          "rating": 4.5,
          "content": "Not Bad. Not Bad at all!",
          "product_id": 123,
          "created_at": "2015-03-27",
          "author": {
            "first_name": "Average",
            "last_name": "Doe",
            "email": "avgdoe@example.com"
          }
        }
      },
      ...
    ]
  }
}

 - All fields in OpenSearch are optional
 - We can leave out a field when indexing docs
 - Some integrity checks need to be done at the applicaiton level (e.g. having required fields)
 - OpenSearch will validate field values against any mapping that might exist, but it won't reject docs that leave fields out.
 - Adding a field mapping does not make a field required
 - Searching API automatically handle missing fields

=================================================================

Overview of mapping parameters

https://docs.opensearch.org/latest/field-types/mapping-parameters/index/

 - format - Specifies the date format for date fields. There is no default value for this parameter. Allowed values are any valid date format string, such as yyyy-MM-dd or epoch_millis.
	Recommended to use default format
		- "strict_date_optional_time||epoch_millis"
	Using Java's DateFormatter syntax
		- "dd/MM/yyyy"
	Using built-in formats
		- "epoch_second"

Examples:
PUT /sales
{
	"mappings": {
		"properties": {
			"purchased_at": {
				"type": "date",
				"format": "dd/MM/yyyy"
			}
		}
	}
}

PUT /sales
{
	"mappings": {
		"properties": {
			"purchased_at": {
				"type": "date",
				"format": "epoch_second"
			}
		}
	}
}

 - properties - defines nested fields for object and nested fields

Examples:
PUT /products
{
  "mappings": {
    "properties": {
      "name": {
        "type": "text"
      },
      "sku": {
        "type": "keyword"
      },
      "price": {
        "type": "float"
      },
      "available": {
        "type": "boolean"
      },
      "created_at": {
        "type": "date",
        "format": "yyyy-MM-dd"
      },
      "dimensions": {
        "type": "object",
        "properties": {
          "width": { "type": "float" },
          "height": { "type": "float" },
          "depth": { "type": "float" }
        }
      }
    }
  }
}

PUT /products/_doc/1
{
  "name": "Wireless Mouse",
  "sku": "WM-1001",
  "price": 24.99,
  "available": true,
  "created_at": "2024-12-01",
  "dimensions": {
    "width": 6.5,
    "height": 3.2,
    "depth": 1.5
  }
}

 - coerce - controls how values are converted to the expected field data type during indexing. 
	This parameter lets you verify that your data is formatted and indexed properly, following the expected field types.
	This improves the accuracy of your search results.
	Used to enable or disable coercion of values (enabled by default)

Examples:
Indexing a document with coerce enabled:
PUT products
{
  "mappings": {
    "properties": {
      "price": {
        "type": "integer",
        "coerce": true
      }
    }
  }
}

PUT products/_doc/1
{
  "name": "Product A",
  "price": "19.99"  // In this example, the price field is defined as an integer type with coerce set to true. When indexing the document, the string value 19.99 is coerced to the integer 19.
}

Indexing a document with coerce disabled:
PUT orders
{
  "mappings": {
    "properties": {
      "quantity": {
        "type": "integer",
        "coerce": false
      }
    }
  }
}

PUT orders/_doc/1
{
  "item": "Widget",
  "quantity": "10" // When indexing the document, the string value 10 is not coerced, and the document is rejected because of the type mismatch.
}

Setting the index-level coercion setting
PUT inventory
{
  "settings": {
    "index.mapping.coerce": false  // the index-level index.mapping.coerce setting is set to false,  which disables coercion for the index
  },
  "mappings": {
    "properties": {
      "stock_count": {
        "type": "integer",
        "coerce": true // the stock_count field overrides this setting and enables coercion for this specific field.
      },
      "sku": {
        "type": "keyword"
      }
    }
  }
}

PUT inventory/_doc/1
{
  "sku": "ABC123",
  "stock_count": "50"
}

 - doc_values - enables document-to-term lookups for operations such as sorting, aggregations, and scripting.
	OpenSearch indexes most fields for search purposes.
	The doc_values parameter is not supported for use in text fields.
Option	Description
true	Enables doc_values for the field. Default is true.
false	Disables doc_values for the field.

Example (The following example request creates an index with doc_values enabled for one field and disabled for another:)
PUT my-index-001
{
  "mappings": {
    "properties": {
      "status_code": { 
        "type": "keyword"
      },
      "session_id": { 
        "type": "keyword",
        "doc_values": false
      }
    }
  }
}


The doc_values parameter is not supported for use in text fields like Inverted Indices
"doc_vaues" is another data structure used by Apache Lucene
"doc_values" optimized for a different data access pattern (document -> terms)

Essentially an "uninverted" inverted index
Set the doc_values to "false" to save disk space if we won't use aggregations, sorting, scripting
If we disable "doc_values", we can not change this without re-indexing (Reindex API)

 - norms - controls whether normalization factors are computed and stored for a field.
	These factors are used during query scoring to adjust the relevance of the search results.
	However, storing norms increases the index size and consumes additional memory.
	By default, norms is enabled on text fields, for which relevance scoring is important.
	Fields that do not require these scoring features, such as keyword fields used only for filtering, are configured with norms disabled.
	Often we don't want just to filter results, but also rank them based on how well they match a given query.

Disabling norms on a field
The following request creates an index named products with the description field as a text field with norms disabled:
PUT /products
{
  "mappings": {
    "properties": {
      "description": {
        "type": "text",
        "norms": false
      }
    }
  }
}
To disable norms on a field in an existing index, use the following request:
PUT /products/_mapping
{
  "properties": {
    "review": {
      "type": "text",
      "norms": false
    }
  }
}

Enabling norms on a field that has norms disabled is impossible and will result in the following error:
{
  "error": {
    "root_cause": [
      {
        "type": "illegal_argument_exception",
        "reason": "Mapper for [description] conflicts with existing mapper:\n\tCannot update parameter [norms] from [false] to [true]"
      }
    ],
    "type": "illegal_argument_exception",
    "reason": "Mapper for [description] conflicts with existing mapper:\n\tCannot update parameter [norms] from [false] to [true]"
  },
  "status": 400
}

 - index - controls whether a field is searchable by including it in the inverted index. 
	When set to true, the field is indexed and available for queries. 
	When set to false, the field is stored in the document (_source) but not indexed, making it non-searchable.
	If you do not need to search a particular field, disabling indexing for that field can reduce index size and improve indexing performance.
	For example, you can disable indexing on large text fields or metadata that is only used for display.
	
Enabling indexing on a field
The following request creates an index named products with a description field that is indexed (the default behavior):
PUT /products
{
  "mappings": {
    "properties": {
      "description": {
        "type": "text"
      }
    }
  }
}

Index a document using the following request:
PUT /products/_doc/1
{
  "description": "This product has a searchable description."
}

Query the description field:
POST /products/_search
{
  "query": {
    "match": {
      "description": "searchable"
    }
  }
}

The following response confirms that the indexed document was successfully matched by the query:
{
  ...
  "hits": {
    "total": {
      "value": 1,
      "relation": "eq"
    },
    "max_score": 0.2876821,
    "hits": [
      {
        "_index": "products",
        "_id": "1",
        "_score": 0.2876821,
        "_source": {
          "description": "This product has a searchable description."
        }
      }
    ]
  }
}

Disabling indexing on a field
Create an index named products-no-index with a description field that is not indexed:

PUT /products-no-index
{
  "mappings": {
    "properties": {
      "description": {
        "type": "text",
        "index": false
      }
    }
  }
}

Index a document using the following request:

PUT /products-no-index/_doc/1
{
  "description": "This product has a non-searchable description."
}

Query products-no-index using the description field:

POST /products-no-index/_search
{
  "query": {
    "match": {
      "description": "non-searchable"
    }
  }
}

The following error response indicates that the search query failed because the description field is not indexed:

{
  "error": {
    "root_cause": [
      {
        "type": "query_shard_exception",
        "reason": "failed to create query: Cannot search on field [description] since it is not indexed.",
        "index": "products-no-index",
        "index_uuid": "yX2F4En1RqOBbf3YWihGCQ"
      }
    ],
    "type": "search_phase_execution_exception",
    "reason": "all shards failed",
    "phase": "query",
    "grouped": true,
    "failed_shards": [
      {
        "shard": 0,
        "index": "products-no-index",
        "node": "0tmy2tf7TKW8qCmya9sG2g",
        "reason": {
          "type": "query_shard_exception",
          "reason": "failed to create query: Cannot search on field [description] since it is not indexed.",
          "index": "products-no-index",
          "index_uuid": "yX2F4En1RqOBbf3YWihGCQ",
          "caused_by": {
            "type": "illegal_argument_exception",
            "reason": "Cannot search on field [description] since it is not indexed."
          }
        }
      }
    ]
  },
  "status": 400
}

 - null_value - allows you to replace explicit null values with a predefined substitute during indexing.
	By default, if a field is set to null, it is not indexed and cannot be searched. 
	With null_value defined, the specified replacement value is indexed instead. 
	This allows you to query or aggregate documents in which a field was originally null without modifying the document _source.
	The null_value must be of the same type as the field it is applied to. 
		For instance, a date field cannot use a boolean such as true as its null_value; the null_value must be a valid date string.

Setting a null_value on a field
The following request creates an index named products. The category field is of type keyword and replaces null values with "unknown" during indexing:
PUT /products
{
  "mappings": {
    "properties": {
      "category": {
        "type": "keyword",
        "null_value": "unknown"
      }
    }
  }
}


Indexing a document with a null value
Use the following command to index a document in which the category field is set to null:
PUT /products/_doc/1
{
  "category": null
}

Querying the null substitute
Use the following command to search for documents in which the category field was previously null:
POST /products/_search
{
  "query": {
    "term": {
      "category": "unknown"
    }
  }
}
The response contains the matching document:
{
  ...
  "hits": {
    "total": {
      "value": 1,
      "relation": "eq"
    },
    "max_score": 0.2876821,
    "hits": [
      {
        "_index": "products",
        "_id": "1",
        "_score": 0.2876821,
        "_source": {
          "category": null
        }
      }
    ]
  }
}

 - copy_to - allows you to copy the values of multiple fields into a single field.
	This parameter can be useful if you often search across multiple fields because it allows you to search the group field instead.
	Only the field value is copied and not the terms resulting from the analysis process.
	The original _source field remains unmodified, and the same value can be copied to multiple fields using the copy_to parameter. 
	However, recursive copying through intermediary fields is not supported; instead, use copy_to directly from the originating field to multiple target fields.

Examples
The following example uses the copy_to parameter to search for products by their name and description and copy those values into a single field:
PUT my-products-index
{
  "mappings": {
    "properties": {
      "name": {
        "type": "text",
        "copy_to": "product_info"
      },
      "description": {
        "type": "text",
        "copy_to": "product_info" 
      },
      "product_info": {
        "type": "text"
      },
      "price": {
        "type": "float"
      }
    }
  }
}

PUT my-products-index/_doc/1
{
  "name": "Wireless Headphones",
  "description": "High-quality wireless headphones with noise cancellation",
  "price": 99.99
}

PUT my-products-index/_doc/2
{
  "name": "Bluetooth Speaker",
  "description": "Portable Bluetooth speaker with long battery life",
  "price": 49.99
}

In this example, the values from the name and description fields are copied into the product_info field. You can now search for products by querying the product_info field, as follows:
GET my-products-index/_search
{
  "query": {
    "match": {
      "product_info": "wireless headphones"
    }
  }
}

esponse
{
  "took": 20,
  "timed_out": false,
  "_shards": {
    "total": 1,
    "successful": 1,
    "skipped": 0,
    "failed": 0
  },
  "hits": {
    "total": {
      "value": 1,
      "relation": "eq"
    },
    "max_score": 1.9061546,
    "hits": [
      {
        "_index": "my-products-index",
        "_id": "1",
        "_score": 1.9061546,
        "_source": {
          "name": "Wireless Headphones",
          "description": "High-quality wireless headphones with noise cancellation",
          "price": 99.99
        }
      }
    ]
  }
}

=================================================================

Update existing mapping

